name: CI/CD Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:  # Allow manual triggers

env:
  PYTHON_VERSION: "3.11"
  TERRAFORM_VERSION: "1.6.0"

jobs:
  # Lint and format check
  lint:
    name: Lint & Format Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Check code formatting with Black
        run: black --check ingestion/ tests/
        continue-on-error: true  # Don't fail build on formatting

      - name: Lint with flake8
        run: |
          # stop the build if there are Python syntax errors or undefined names
          flake8 ingestion/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings
          flake8 ingestion/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
        continue-on-error: true  # Don't fail build on lint warnings

      - name: Type check with mypy
        run: mypy ingestion/ --ignore-missing-imports
        continue-on-error: true  # Don't fail build on type errors yet

      - name: Security scan with bandit
        run: bandit -r ingestion/ -ll
        continue-on-error: true  # Don't fail build on security warnings yet

  # Unit tests
  test-unit:
    name: Unit Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ingestion/requirements.txt
          pip install -r requirements-dev.txt

      - name: Run unit tests with coverage
        run: |
          pytest tests/unit/ \
            --cov=ingestion \
            --cov-report=xml \
            --cov-report=term \
            --cov-fail-under=0 \
            -v
        continue-on-error: true  # Don't fail build on test failures during development

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
        continue-on-error: true

  # Terraform validation
  terraform-validate:
    name: Terraform Validation
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Format Check
        run: terraform fmt -check -recursive infra/terraform/
        continue-on-error: true

      - name: Terraform Init
        run: |
          cd infra/terraform
          terraform init

      - name: Terraform Validate
        run: |
          cd infra/terraform
          terraform validate

  # Build Lambda packages (on main branch only)
  build-lambdas:
    name: Build Lambda Packages
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: [lint, test-unit, terraform-validate]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Package ALL Lambda Functions
        run: |
          # Use Makefile to package all Lambdas consistently
          make package-all

      - name: Upload Lambda artifacts
        uses: actions/upload-artifact@v4
        with:
          name: lambda-packages
          path: ingestion/lambdas/*/function.zip
          retention-days: 30

  # Deploy (auto-deploys on push to main, or manual trigger)
  deploy:
    name: Deploy to AWS
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: [build-lambdas]
    environment:
      name: production
      url: https://console.aws.amazon.com

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Verify AWS credentials and permissions
        run: |
          echo "ğŸ” Verifying AWS credentials and permissions..."
          echo "AWS Account ID:"
          aws sts get-caller-identity --query 'Account' --output text || {
            echo "âŒ Failed to get AWS account identity. Check credentials."
            exit 1
          }
          echo "âœ… AWS credentials are valid"
          
          # Check basic permissions
          echo "ğŸ” Checking required permissions..."
          echo "  - S3 access:"
          aws s3 ls 2>/dev/null && echo "    âœ… S3 access OK" || echo "    âš ï¸  S3 access check failed"
          echo "  - DynamoDB access:"
          aws dynamodb list-tables --region us-east-1 2>/dev/null && echo "    âœ… DynamoDB access OK" || echo "    âš ï¸  DynamoDB access check failed"
          
          # Required IAM permissions for this workflow:
          # - s3:CreateBucket, s3:PutBucketVersioning, s3:PutBucketEncryption, s3:GetBucketLocation
          # - dynamodb:CreateTable, dynamodb:DescribeTable, dynamodb:PutItem, dynamodb:DeleteItem, dynamodb:ListTables
          # - terraform state: s3:GetObject, s3:PutObject, s3:ListBucket on state bucket
          # - dynamodb:PutItem, dynamodb:GetItem, dynamodb:DeleteItem on lock table

      - name: Ensure S3 state bucket exists
        run: |
          set -e  # Exit on error
          BUCKET_NAME="congress-disclosures-standardized"
          REGION="us-east-1"
          STATE_KEY_PREFIX="terraform/development/"
          
          echo "ğŸ” Checking if S3 state bucket exists: $BUCKET_NAME"
          
          if aws s3api head-bucket --bucket "$BUCKET_NAME" 2>/dev/null; then
            echo "âœ… S3 state bucket already exists: $BUCKET_NAME"
            
            # Verify versioning is enabled
            VERSIONING=$(aws s3api get-bucket-versioning --bucket "$BUCKET_NAME" --query 'Status' --output text 2>/dev/null || echo "None")
            if [ "$VERSIONING" != "Enabled" ]; then
              echo "âš ï¸  Versioning not enabled, enabling now..."
              aws s3api put-bucket-versioning \
                --bucket "$BUCKET_NAME" \
                --versioning-configuration Status=Enabled || {
                echo "âŒ Failed to enable versioning"
                exit 1
              }
              echo "âœ… Versioning enabled"
            else
              echo "âœ… Versioning is already enabled"
            fi
            
            # Verify encryption
            ENCRYPTION=$(aws s3api get-bucket-encryption --bucket "$BUCKET_NAME" --query 'ServerSideEncryptionConfiguration.Rules[0].ApplyServerSideEncryptionByDefault.SSEAlgorithm' --output text 2>/dev/null || echo "None")
            if [ "$ENCRYPTION" != "AES256" ]; then
              echo "âš ï¸  Encryption not configured, enabling now..."
              aws s3api put-bucket-encryption \
                --bucket "$BUCKET_NAME" \
                --server-side-encryption-configuration '{
                  "Rules": [{
                    "ApplyServerSideEncryptionByDefault": {
                      "SSEAlgorithm": "AES256"
                    }
                  }]
                }' || {
                echo "âŒ Failed to enable encryption"
                exit 1
              }
              echo "âœ… Encryption enabled"
            else
              echo "âœ… Encryption is already configured"
            fi
          else
            echo "ğŸ“¦ Creating S3 bucket for Terraform state: $BUCKET_NAME"
            # us-east-1 doesn't need LocationConstraint
            aws s3api create-bucket \
              --bucket "$BUCKET_NAME" \
              --region "$REGION" || {
              echo "âŒ Failed to create S3 bucket. Checking permissions..."
              aws sts get-caller-identity
              exit 1
            }
            
            # Enable versioning for state bucket (critical for state management)
            echo "ğŸ“¦ Enabling versioning on state bucket..."
            aws s3api put-bucket-versioning \
              --bucket "$BUCKET_NAME" \
              --versioning-configuration Status=Enabled || {
              echo "âŒ Failed to enable versioning"
              exit 1
            }
            
            # Enable encryption
            echo "ğŸ” Enabling encryption on state bucket..."
            aws s3api put-bucket-encryption \
              --bucket "$BUCKET_NAME" \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }]
              }' || {
              echo "âŒ Failed to enable encryption"
              exit 1
            }
            
            echo "âœ… S3 state bucket created and configured"
          fi
          
          # Verify we can access the state path
          echo "ğŸ” Verifying state path access: s3://$BUCKET_NAME/$STATE_KEY_PREFIX"
          aws s3 ls "s3://$BUCKET_NAME/$STATE_KEY_PREFIX" 2>/dev/null && echo "âœ… State path is accessible" || echo "â„¹ï¸  State path doesn't exist yet (will be created by terraform init)"

      - name: Ensure DynamoDB lock table exists
        run: |
          set -e  # Exit on error
          TABLE_NAME="congress-disclosures-terraform-locks"
          REGION="us-east-1"
          
          echo "ğŸ” Checking if DynamoDB lock table exists: $TABLE_NAME"
          
          # Check if table exists and is accessible
          if aws dynamodb describe-table --table-name "$TABLE_NAME" --region "$REGION" 2>/dev/null; then
            TABLE_STATUS=$(aws dynamodb describe-table --table-name "$TABLE_NAME" --region "$REGION" --query 'Table.TableStatus' --output text)
            echo "âœ… DynamoDB lock table exists with status: $TABLE_STATUS"
            
            if [ "$TABLE_STATUS" != "ACTIVE" ]; then
              echo "â³ Waiting for table to become ACTIVE..."
              aws dynamodb wait table-exists --table-name "$TABLE_NAME" --region "$REGION"
              echo "âœ… Table is now ACTIVE"
            fi
          else
            echo "ğŸ“¦ Creating DynamoDB lock table: $TABLE_NAME"
            aws dynamodb create-table \
              --table-name "$TABLE_NAME" \
              --region "$REGION" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --tags Key=Name,Value="Terraform State Lock Table" \
                     Key=Project,Value="congress-disclosures" || {
              echo "âŒ Failed to create DynamoDB table. Checking permissions..."
              aws sts get-caller-identity
              exit 1
            }
            
            echo "â³ Waiting for table to become active..."
            aws dynamodb wait table-exists --table-name "$TABLE_NAME" --region "$REGION" || {
              echo "âŒ Table creation timed out or failed"
              exit 1
            }
            
            # Verify table is ACTIVE
            TABLE_STATUS=$(aws dynamodb describe-table --table-name "$TABLE_NAME" --region "$REGION" --query 'Table.TableStatus' --output text)
            if [ "$TABLE_STATUS" = "ACTIVE" ]; then
              echo "âœ… DynamoDB lock table created and ACTIVE"
            else
              echo "âŒ Table exists but status is $TABLE_STATUS (expected ACTIVE)"
              exit 1
            fi
          fi
          
          # Final verification - test write access
          echo "ğŸ” Verifying table access permissions..."
          TEST_LOCK_ID="test-lock-$(date +%s)"
          aws dynamodb put-item \
            --table-name "$TABLE_NAME" \
            --region "$REGION" \
            --item "{\"LockID\": {\"S\": \"$TEST_LOCK_ID\"}}" \
            --condition-expression "attribute_not_exists(LockID)" 2>/dev/null && \
          aws dynamodb delete-item \
            --table-name "$TABLE_NAME" \
            --region "$REGION" \
            --key "{\"LockID\": {\"S\": \"$TEST_LOCK_ID\"}}" 2>/dev/null && \
          echo "âœ… Table access verified (read/write permissions OK)" || {
            echo "âš ï¸  Warning: Could not verify write access to table (may still work for Terraform)"
          }

      - name: Download Lambda artifacts
        uses: actions/download-artifact@v4
        with:
          name: lambda-packages
          path: ingestion/lambdas/

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Init
        run: |
          cd infra/terraform
          terraform init -upgrade
          echo "âœ… Terraform backend initialized successfully"

      - name: Verify Terraform Backend
        run: |
          cd infra/terraform
          # Verify backend is configured correctly
          terraform workspace show || echo "Using default workspace"
          echo "âœ… Backend verification complete"

      - name: Terraform Plan
        run: |
          cd infra/terraform
          terraform plan -out=tfplan \
            -var="environment=development" \
            -var="s3_bucket_name=${{ secrets.S3_BUCKET_NAME }}" \
            -var="budget_alert_email=${{ secrets.BUDGET_ALERT_EMAIL }}"

      - name: Import existing resources into Terraform state
        env:
          ENVIRONMENT: development
        run: |
          chmod +x scripts/terraform_import.sh
          ./scripts/terraform_import.sh


      - name: Terraform Apply
        run: |
          cd infra/terraform
          terraform apply tfplan

      - name: Upload API Keys to SSM Parameter Store
        run: |
          ENV="development"
          echo "ğŸ“¦ Uploading API keys to SSM Parameter Store..."

          # Congress API Key
          aws ssm put-parameter \
            --name "/congress-disclosures/${ENV}/congress_api_key" \
            --value "${{ secrets.CONGRESS_API_KEY }}" \
            --type "SecureString" \
            --overwrite || echo "âš ï¸  Failed to update Congress API key"

          # Alpha Vantage API Key
          aws ssm put-parameter \
            --name "/congress-disclosures/${ENV}/alpha_vantage_api_key" \
            --value "${{ secrets.ALPHA_VANTAGE_API_KEY }}" \
            --type "SecureString" \
            --overwrite || echo "âš ï¸  Failed to update Alpha Vantage API key"

          # Coinbase API Key
          aws ssm put-parameter \
            --name "/congress-disclosures/${ENV}/coinbase_api_key" \
            --value "${{ secrets.COINBASE_API_KEY }}" \
            --type "SecureString" \
            --overwrite || echo "âš ï¸  Failed to update Coinbase API key"

          # Coinbase API Secret
          aws ssm put-parameter \
            --name "/congress-disclosures/${ENV}/coinbase_api_secret" \
            --value "${{ secrets.COINBASE_API_SECRET }}" \
            --type "SecureString" \
            --overwrite || echo "âš ï¸  Failed to update Coinbase API secret"

          echo "âœ… API keys uploaded to SSM"

      - name: Upload Lambda packages to S3
        run: |
          for lambda_dir in ingestion/lambdas/*/; do
            lambda_name=$(basename $lambda_dir)
            if [ -f "$lambda_dir/function.zip" ]; then
              echo "Uploading $lambda_name to S3..."
              aws s3 cp "$lambda_dir/function.zip" \
                "s3://congress-disclosures-standardized/lambda-deployments/${lambda_name}/function.zip"
            fi
          done

      - name: Update Lambda function codes
        run: |
          ENV="development"
          for lambda_dir in ingestion/lambdas/*/; do
            lambda_name=$(basename $lambda_dir)
            if [ -f "$lambda_dir/function.zip" ]; then
              echo "Updating Lambda: congress-disclosures-${ENV}-${lambda_name}"
              aws lambda update-function-code \
                --function-name "congress-disclosures-${ENV}-${lambda_name}" \
                --s3-bucket congress-disclosures-standardized \
                --s3-key "lambda-deployments/${lambda_name}/function.zip" \
                || echo "âš ï¸  Lambda $lambda_name not found or update failed"
            fi
          done

      - name: Run ETL Pipeline (Silver+Gold)
        run: |
          echo "ğŸ”„ Running ETL Pipeline (Silver -> Gold)..."
          python3 -m pip install boto3 pandas requests pyarrow
          
          # Run for 2025 (and potentially 2024 if needed)
          python3 scripts/run_etl_pipeline.py \
            --mode silver_gold \
            --years 2025 \
            --environment development

      - name: Refresh Gold Layer Aggregates
        run: |
          echo "ğŸ”„ Refreshing Gold layer aggregates..."
          python3 -m pip install boto3 pandas pyarrow
          python3 scripts/compute_agg_member_trading_stats_real.py || echo "Member stats refresh failed"
          # Add other gold aggregates here as you create them

      - name: Regenerate Website Manifests
        run: |
          echo "ğŸ“Š Regenerating website data manifests..."
          python3 scripts/generate_all_gold_manifests.py

      - name: Deploy Website to S3
        run: |
          echo "ğŸŒ Deploying website..."
          aws s3 sync website/ s3://congress-disclosures-standardized/website/ \
            --exclude "data/*" \
            --cache-control "no-cache, no-store, must-revalidate" \
            --delete

          aws s3 sync website/data/ s3://congress-disclosures-standardized/website/data/ \
            --cache-control "no-cache, no-store, must-revalidate" \
            --delete

          echo "âœ… Full pipeline deployment complete!"
