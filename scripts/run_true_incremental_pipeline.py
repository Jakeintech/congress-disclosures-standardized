#!/usr/bin/env python3
"""
True Incremental Pipeline Runner

A proper incremental pipeline that:
1. Tracks last run state (timestamps)
2. Only fetches NEW data since last run
3. Only processes files that haven't been processed yet
4. Runs Silver ‚Üí Gold transformations only for changed data
5. Handles all 3 data sources: House FD, Congress, Lobbying

Usage:
    python scripts/run_true_incremental_pipeline.py [--year 2025] [--force]
"""

import boto3
import argparse
import sys
import json
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Optional

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

# Configuration
BUCKET = "congress-disclosures-standardized"
STATE_FILE_KEY = "pipeline_state/last_incremental_run.json"
DEFAULT_YEAR = 2025

s3 = boto3.client('s3')
lambda_client = boto3.client('lambda')
sqs = boto3.client('sqs')


def print_banner(title: str):
    """Print section banner."""
    print(f"\n{'='*80}")
    print(f"{title}")
    print(f"{'='*80}\n")


def load_pipeline_state() -> Dict:
    """Load state from last pipeline run."""
    try:
        response = s3.get_object(Bucket=BUCKET, Key=STATE_FILE_KEY)
        state = json.loads(response['Body'].read().decode('utf-8'))
        print(f"üìÖ Last incremental run: {state.get('timestamp', 'unknown')}")
        return state
    except s3.exceptions.NoSuchKey:
        print("‚ÑπÔ∏è  No previous run state found (first run)")
        return {}
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not load state: {e}")
        return {}


def save_pipeline_state(state: Dict):
    """Save pipeline state for next incremental run."""
    state['timestamp'] = datetime.now(timezone.utc).isoformat()

    try:
        s3.put_object(
            Bucket=BUCKET,
            Key=STATE_FILE_KEY,
            Body=json.dumps(state, indent=2),
            ContentType='application/json'
        )
        print(f"‚úÖ Pipeline state saved: {STATE_FILE_KEY}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not save state: {e}")


def count_unprocessed_pdfs(year: int) -> int:
    """Count PDFs in Bronze that haven't been extracted yet."""
    print(f"üîç Checking Bronze layer for unprocessed PDFs (year={year})...")

    prefix = f"bronze/house/financial/year={year}/"
    unprocessed = 0
    total = 0

    try:
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=BUCKET, Prefix=prefix)

        for page in pages:
            if 'Contents' not in page:
                continue

            for obj in page['Contents']:
                if obj['Key'].endswith('.pdf'):
                    total += 1

                    # Check if processed by looking at metadata
                    try:
                        head = s3.head_object(Bucket=BUCKET, Key=obj['Key'])
                        metadata = head.get('Metadata', {})

                        if metadata.get('extraction-processed') != 'true':
                            unprocessed += 1
                    except:
                        unprocessed += 1  # Assume unprocessed if can't check

        print(f"   Total PDFs: {total}")
        print(f"   Unprocessed: {unprocessed}")
        print(f"   Already extracted: {total - unprocessed}")

        return unprocessed

    except Exception as e:
        print(f"‚ö†Ô∏è  Error counting PDFs: {e}")
        return 0


def run_house_fd_incremental(year: int, last_run: Optional[str]) -> Dict:
    """Run incremental House FD ingestion."""
    print_banner("üì• House Financial Disclosures - Incremental Update")

    print(f"üìÖ Year: {year}")
    if last_run:
        print(f"üìÖ Last run: {last_run}")
    print(f"üîß Mode: Incremental (skip existing files)\n")

    # Check current state before ingestion
    unprocessed_before = count_unprocessed_pdfs(year)

    print(f"\n‚è≥ Invoking ingestion Lambda (skip_existing=True)...")

    payload = {
        "year": year,
        "skip_existing": True
    }

    try:
        response = lambda_client.invoke(
            FunctionName="congress-disclosures-development-ingest-zip",
            InvocationType='RequestResponse',
            Payload=json.dumps(payload)
        )

        result = json.loads(response['Payload'].read())

        if result.get('status') == 'success':
            new_pdfs = result.get('pdfs_uploaded', 0)
            skipped = result.get('pdfs_skipped', 0)

            print(f"\n‚úÖ Ingestion complete:")
            print(f"   ‚Ä¢ NEW files: {new_pdfs}")
            print(f"   ‚Ä¢ Skipped (already in Bronze): {skipped}")
            print(f"   ‚Ä¢ Total in index: {result.get('index_entry_count', 'N/A')}")

            # Trigger Silver processing ONLY if there are new files OR unprocessed files
            should_process_silver = (new_pdfs > 0) or (unprocessed_before > 0)

            if should_process_silver:
                print(f"\n‚è≥ Triggering Silver layer processing...")
                print(f"   Reason: {new_pdfs} new + {unprocessed_before} unprocessed = {new_pdfs + unprocessed_before} to extract")

                silver_response = lambda_client.invoke(
                    FunctionName="congress-disclosures-development-index-to-silver",
                    InvocationType='RequestResponse',
                    Payload=json.dumps({"year": year})
                )

                silver_result = json.loads(silver_response['Payload'].read())
                print(f"‚úÖ Silver processing triggered: {silver_result.get('documents_initialized', 'N/A')} docs")
            else:
                print(f"\n‚ÑπÔ∏è  No new files and no unprocessed files - skipping Silver processing")

            return {
                'status': 'success',
                'new_files': new_pdfs,
                'should_wait_extraction': should_process_silver
            }
        else:
            print(f"‚ùå Ingestion failed: {result}")
            return {'status': 'error', 'should_wait_extraction': False}

    except Exception as e:
        print(f"‚ùå Failed: {e}")
        return {'status': 'error', 'should_wait_extraction': False}


def run_congress_incremental(last_run: Optional[str]) -> Dict:
    """Run incremental Congress bills/members update."""
    print_banner("üèõÔ∏è  Congress Bills & Members - Incremental Update")

    if last_run:
        print(f"üìÖ Fetching updates since: {last_run}")
    else:
        print(f"üìÖ First run - will fetch recent bills")

    print(f"‚è≥ Triggering orchestrator for: member, bill\n")

    entity_types = ['member', 'bill']
    results = {}

    for entity_type in entity_types:
        print(f"üì¶ {entity_type}...")

        payload = {
            'entity_type': entity_type,
            'mode': 'incremental'
        }

        try:
            response = lambda_client.invoke(
                FunctionName="congress-disclosures-development-congress-orchestrator",
                InvocationType='Event',  # Async
                Payload=json.dumps(payload)
            )

            if response.get('StatusCode') in [200, 202]:
                print(f"   ‚úÖ Queued for async processing")
                results[entity_type] = 'queued'
            else:
                print(f"   ‚ö†Ô∏è  May have failed")
                results[entity_type] = 'unknown'

        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            results[entity_type] = 'error'

    print(f"\n‚ÑπÔ∏è  Congress data fetched asynchronously - Silver processing happens automatically")
    print(f"‚ÑπÔ∏è  Bill actions & cosponsors will be updated in Gold layer aggregation step")

    return {'status': 'success', 'results': results}


def run_lobbying_incremental(year: int, last_run: Optional[str]) -> Dict:
    """Run incremental Lobbying data update."""
    print_banner("üí∞ Lobbying Data - Incremental Update")

    print(f"üìÖ Year: {year}")
    if last_run:
        print(f"üìÖ Fetching filings since: {last_run}")
    else:
        print(f"üìÖ First run - will fetch recent filings")

    print(f"‚è≥ Triggering LDA ingestion...\n")

    # For now, just check if Bronze data exists
    try:
        response = s3.list_objects_v2(
            Bucket=BUCKET,
            Prefix=f"bronze/lobbying/filings/year={year}/",
            MaxKeys=10
        )

        if 'Contents' in response:
            count = len(response['Contents'])
            print(f"‚úÖ Found {count} existing filings in Bronze")
            print(f"‚ÑπÔ∏è  Lobbying incremental update would fetch new filings here")
            print(f"‚ÑπÔ∏è  (Currently requires manual trigger via make ingest-lobbying)")
        else:
            print(f"‚ö†Ô∏è  No lobbying data found - run full ingestion first:")
            print(f"    make ingest-lobbying-all YEAR={year}")

        return {'status': 'success'}

    except Exception as e:
        print(f"‚ö†Ô∏è  Error checking lobbying data: {e}")
        return {'status': 'warning'}


def wait_for_extraction_smart(timeout_minutes: int = 30) -> bool:
    """Wait for extraction queue, but with smart timeout for incremental runs."""
    print_banner("‚è≥ Waiting for Extraction Processing")

    try:
        queue_url = sqs.get_queue_url(
            QueueName="congress-disclosures-development-extract-queue"
        )['QueueUrl']
    except:
        print("‚ö†Ô∏è  Could not find extraction queue")
        return True  # Continue anyway

    print(f"Monitoring queue (timeout: {timeout_minutes}m)...\n")

    start_time = time.time()
    last_count = -1
    no_change_cycles = 0

    while (time.time() - start_time) < (timeout_minutes * 60):
        try:
            response = sqs.get_queue_attributes(
                QueueUrl=queue_url,
                AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
            )

            visible = int(response['Attributes'].get('ApproximateNumberOfMessages', 0))
            inflight = int(response['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
            total = visible + inflight

            if total == 0:
                print("\n‚úÖ Queue drained - extraction complete")
                time.sleep(30)  # Wait for eventual consistency
                return True

            # Smart timeout: if queue isn't changing, something may be stuck
            if total == last_count:
                no_change_cycles += 1
                if no_change_cycles > 10:  # 100 seconds of no change
                    print(f"\n‚ö†Ô∏è  Queue stuck at {total} messages for 100s - continuing anyway")
                    return True
            else:
                no_change_cycles = 0

            elapsed = int(time.time() - start_time)
            print(f"\r‚è≥ Queue: {total:,} ({visible:,} waiting, {inflight:,} processing) | {elapsed}s elapsed   ",
                  end="", flush=True)

            last_count = total
            time.sleep(10)

        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error: {e}")
            time.sleep(10)

    print(f"\n‚ö†Ô∏è  Timeout after {timeout_minutes}m - continuing anyway")
    return True  # Don't fail the pipeline


def run_aggregations():
    """Run aggregation pipeline (calls the existing smart pipeline aggregate mode)."""
    print_banner("üèóÔ∏è  Running Gold Layer Aggregations")

    print("‚è≥ Executing run_smart_pipeline.py --mode aggregate...\n")

    import subprocess
    result = subprocess.run(
        ["python3", "scripts/run_smart_pipeline.py", "--mode", "aggregate"],
        capture_output=False  # Stream output
    )

    return result.returncode == 0


def main():
    parser = argparse.ArgumentParser(description="True Incremental Pipeline Runner")
    parser.add_argument("--year", type=int, default=DEFAULT_YEAR, help="Year to process")
    parser.add_argument("--force", action="store_true", help="Force full re-processing")
    args = parser.parse_args()

    # Print banner
    print("\n" + "="*80)
    print("üöÄ TRUE INCREMENTAL PIPELINE RUNNER".center(80))
    print("="*80)
    print(f"üìÖ Year: {args.year}")
    print(f"üïê Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

    start_time = time.time()

    # Load last run state
    print_banner("üìã Loading Pipeline State")
    state = load_pipeline_state() if not args.force else {}
    last_run = state.get('timestamp')

    if args.force:
        print("‚ö†Ô∏è  --force flag set: treating as fresh run")

    # Step 1: House FD Incremental
    fd_result = run_house_fd_incremental(args.year, last_run)

    # Step 2: Congress Incremental
    congress_result = run_congress_incremental(last_run)

    # Step 3: Lobbying Incremental
    lobbying_result = run_lobbying_incremental(args.year, last_run)

    # Step 4: Wait for extraction (only if new House FD files)
    if fd_result.get('should_wait_extraction'):
        wait_for_extraction_smart(timeout_minutes=30)
    else:
        print_banner("‚è© Skipping Extraction Wait")
        print("No new House FD files to extract\n")

    # Step 5: Run aggregations (Silver ‚Üí Gold)
    if not run_aggregations():
        print("‚ùå Aggregation failed")
        return 1

    # Save new state
    new_state = {
        'year': args.year,
        'house_fd': fd_result,
        'congress': congress_result,
        'lobbying': lobbying_result,
        'timestamp': datetime.now(timezone.utc).isoformat()
    }
    save_pipeline_state(new_state)

    # Final summary
    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)

    print("\n" + "="*80)
    print("‚ú® INCREMENTAL PIPELINE COMPLETE!".center(80))
    print("="*80)
    print(f"‚è±Ô∏è  Total time: {minutes}m {seconds}s")
    print(f"üïê Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìä Summary:")
    print(f"   ‚Ä¢ House FD: {fd_result.get('new_files', 0)} new files")
    print(f"   ‚Ä¢ Congress: {len([v for v in congress_result.get('results', {}).values() if v == 'queued'])} entities queued")
    print(f"   ‚Ä¢ Lobbying: {lobbying_result.get('status', 'unknown')}")
    print("="*80)
    print("\n‚ú® Next steps:")
    print("   ‚Ä¢ Run 'make deploy-website' to update frontend")
    print("   ‚Ä¢ Review CloudWatch logs for any issues\n")

    return 0


if __name__ == "__main__":
    sys.exit(main())
